{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from direct.preprocessor import Preprocessor, pose_inv, SceneData\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "\n",
    "dir = \"experiments/wood\"\n",
    "\n",
    "demo_head_rgb = np.array(Image.open(\"../{0}/demo_head_rgb.png\".format(dir)))\n",
    "demo_head_depth = np.array(Image.open(\"../{0}/demo_head_depth.png\".format(dir)))\n",
    "demo_head_mask = np.array(Image.open(\"../{0}/demo_head_seg.png\".format(dir)))\n",
    "\n",
    "live_head_rgb = np.array(Image.open(\"../{0}/live_d415_rgb.png\".format(dir)))\n",
    "live_head_depth = np.array(Image.open(\"../{0}/live_d415_depth.png\".format(dir)))\n",
    "live_head_mask = np.array(Image.open(\"../{0}/live_d415_seg.png\".format(dir)))\n",
    "\n",
    "intrinsics_d415 = np.load(\"../handeye/intrinsics_d415.npy\")\n",
    "intrinsics_d405 = np.load(\"../handeye/intrinsics_d405.npy\")\n",
    "T_WC = np.load(\"../handeye/T_WC_head.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = SceneData(\n",
    "    image_0=demo_head_rgb,\n",
    "    image_1=live_head_rgb,\n",
    "    depth_0=demo_head_depth,\n",
    "    depth_1=live_head_depth,\n",
    "    seg_0=demo_head_mask,\n",
    "    seg_1=live_head_mask,\n",
    "    intrinsics_0=intrinsics_d415,\n",
    "    intrinsics_1=intrinsics_d415,\n",
    "    T_WC=np.eye(4) # np.eye(4) = cam frame\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0371766  0.03764415 0.65259916]\n",
      "[0.0734359  0.04258799 0.6330911 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.0362593 , -0.00494384,  0.01950806], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor = Preprocessor()\n",
    "data.update(processor(data))\n",
    "\n",
    "pcd0 = o3d.geometry.PointCloud()\n",
    "pcd0.points = o3d.utility.Vector3dVector(data[\"pc0\"][:, :3])\n",
    "pcd1 = o3d.geometry.PointCloud()\n",
    "pcd1.points = o3d.utility.Vector3dVector(data[\"pc1\"][:, :3])\n",
    "o3d.visualization.draw_geometries([pcd0, pcd1])\n",
    "\n",
    "# Calculate the centroid of each point cloud\n",
    "pcd0_centre = np.mean(data[\"pc0\"][:, :3], axis=0)  # Calculate mean across columns (axis=0)\n",
    "pcd1_centre = np.mean(data[\"pc1\"][:, :3], axis=0)  # Calculate mean across columns (axis=0)\n",
    "\n",
    "print(pcd0_centre)\n",
    "print(pcd1_centre)\n",
    "# # Compute the difference between the centroids\n",
    "diff = pcd0_centre - pcd1_centre\n",
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 - Roll: 1.416, Pitch: -0.394, Yaw: 0.729, Fitness: 0.000\n",
      "Iteration 2 - Roll: -1.571, Pitch: 1.571, Yaw: -1.571, Fitness: 0.000\n",
      "Iteration 3 - Roll: 1.548, Pitch: -1.466, Yaw: 1.434, Fitness: 0.000\n",
      "Iteration 4 - Roll: -1.536, Pitch: 1.497, Yaw: -1.517, Fitness: 0.000\n",
      "Iteration 5 - Roll: 1.554, Pitch: -1.415, Yaw: 1.515, Fitness: 0.000\n",
      "Iteration 6 - Roll: -1.481, Pitch: -1.561, Yaw: -1.569, Fitness: 0.000\n",
      "Iteration 7 - Roll: 1.565, Pitch: 1.463, Yaw: 1.524, Fitness: 0.000\n",
      "Iteration 8 - Roll: -1.501, Pitch: -1.413, Yaw: -1.555, Fitness: 0.000\n",
      "Iteration 9 - Roll: 1.524, Pitch: 1.542, Yaw: 1.557, Fitness: 0.000\n",
      "Iteration 10 - Roll: -1.541, Pitch: -1.407, Yaw: -1.553, Fitness: 0.000\n",
      "Iteration 11 - Roll: 1.564, Pitch: 1.486, Yaw: 1.425, Fitness: 0.000\n",
      "Iteration 12 - Roll: -1.565, Pitch: -1.476, Yaw: 1.518, Fitness: 0.000\n",
      "Iteration 13 - Roll: -1.453, Pitch: 1.556, Yaw: -1.548, Fitness: 0.000\n",
      "Iteration 14 - Roll: 1.547, Pitch: 1.484, Yaw: -1.502, Fitness: 0.000\n",
      "Iteration 15 - Roll: -1.567, Pitch: -1.493, Yaw: 1.440, Fitness: 0.000\n",
      "Iteration 16 - Roll: 1.544, Pitch: 1.495, Yaw: -1.540, Fitness: 0.000\n",
      "Iteration 17 - Roll: 1.545, Pitch: -1.468, Yaw: 1.461, Fitness: 0.000\n",
      "Iteration 18 - Roll: -1.566, Pitch: 1.516, Yaw: -1.549, Fitness: 0.000\n",
      "Iteration 19 - Roll: 1.516, Pitch: -1.494, Yaw: 1.565, Fitness: 0.000\n",
      "Iteration 20 - Roll: -1.543, Pitch: 1.531, Yaw: -1.519, Fitness: 0.000\n",
      "Iteration 21 - Roll: 1.567, Pitch: -1.417, Yaw: 1.520, Fitness: 0.000\n",
      "Iteration 22 - Roll: -1.537, Pitch: 1.546, Yaw: 1.460, Fitness: 0.000\n",
      "Iteration 23 - Roll: 1.531, Pitch: -1.547, Yaw: -1.476, Fitness: 0.000\n",
      "Iteration 24 - Roll: -1.494, Pitch: 1.520, Yaw: -1.548, Fitness: 0.000\n",
      "Iteration 25 - Roll: -1.532, Pitch: -1.540, Yaw: 1.474, Fitness: 0.000\n",
      "Iteration 26 - Roll: 1.479, Pitch: -1.464, Yaw: -1.555, Fitness: 0.000\n",
      "Iteration 27 - Roll: -1.552, Pitch: 1.560, Yaw: 1.441, Fitness: 0.000\n",
      "Iteration 28 - Roll: 1.503, Pitch: -1.499, Yaw: -1.564, Fitness: 0.000\n",
      "Iteration 29 - Roll: -1.544, Pitch: 1.541, Yaw: 1.437, Fitness: 0.000\n",
      "Iteration 30 - Roll: 1.569, Pitch: -1.564, Yaw: 1.500, Fitness: 0.000\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Open3D ICP was unable to register the two point clouds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 103\u001b[0m\n\u001b[1;32m    100\u001b[0m pcd1\u001b[38;5;241m.\u001b[39mpoints \u001b[38;5;241m=\u001b[39m o3d\u001b[38;5;241m.\u001b[39mutility\u001b[38;5;241m.\u001b[39mVector3dVector(data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpc1\u001b[39m\u001b[38;5;124m\"\u001b[39m][:, :\u001b[38;5;241m3\u001b[39m])\n\u001b[1;32m    102\u001b[0m estimator \u001b[38;5;241m=\u001b[39m Open3dICPPoseEstimator()\n\u001b[0;32m--> 103\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimate_relative_pose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpcd0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpcd1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT_WC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualise_pcds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest transformation:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "Cell \u001b[0;32mIn[36], line 67\u001b[0m, in \u001b[0;36mOpen3dICPPoseEstimator.estimate_relative_pose\u001b[0;34m(self, pcd1_o3d, pcd2_o3d, T_WC, verbose, visualise_pcds)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdraw_registration_result(pcd1_o3d, pcd2_o3d, reg_p2p\u001b[38;5;241m.\u001b[39mtransformation)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reg_p2p\u001b[38;5;241m.\u001b[39mfitness \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 67\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOpen3D ICP was unable to register the two point clouds\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m reg_p2p\u001b[38;5;241m.\u001b[39mtransformation\n",
      "\u001b[0;31mException\u001b[0m: Open3D ICP was unable to register the two point clouds"
     ]
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "from bayes_opt import BayesianOptimization, UtilityFunction\n",
    "import time\n",
    "\n",
    "class Open3dICPPoseEstimator:\n",
    "\n",
    "    def __init__(self,\n",
    "                 max_correspondence_distance: float = 0.10,\n",
    "                 max_iteration: int = 10,\n",
    "                 timeout=5):\n",
    "        self.timeout = timeout\n",
    "        self.max_correspondence_distance = max_correspondence_distance\n",
    "        self.max_iteration = max_iteration\n",
    "\n",
    "    def estimate_relative_pose(self,\n",
    "                               pcd1_o3d: o3d.geometry.PointCloud,\n",
    "                               pcd2_o3d: o3d.geometry.PointCloud,\n",
    "                               T_WC: np.ndarray,\n",
    "                               verbose=True,\n",
    "                               visualise_pcds=False) -> np.ndarray:\n",
    "\n",
    "        pcd1_mean = np.asarray(pcd1_o3d.points).mean(axis=0)\n",
    "        pcd2_mean = np.asarray(pcd2_o3d.points).mean(axis=0)\n",
    "\n",
    "        def objective_function(roll, pitch, yaw):\n",
    "            transformation = self.get_transformation_matrix(roll, pitch, yaw, pcd1_mean, pcd2_mean, T_WC)\n",
    "            reg_p2p = o3d.pipelines.registration.registration_icp(\n",
    "                pcd1_o3d, pcd2_o3d, self.max_correspondence_distance, transformation,\n",
    "                o3d.pipelines.registration.TransformationEstimationPointToPoint(),\n",
    "                o3d.pipelines.registration.ICPConvergenceCriteria(max_iteration=self.max_iteration)\n",
    "            )\n",
    "            return -reg_p2p.fitness  # Negate fitness to convert maximization to minimization\n",
    "\n",
    "        pbounds = {'roll': (-np.pi/2, np.pi/2), 'pitch': (-np.pi/2, np.pi/2), 'yaw': (-np.pi/2, np.pi/2)}\n",
    "\n",
    "        optimizer = BayesianOptimization(\n",
    "            f=objective_function,\n",
    "            pbounds=pbounds,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        utility = UtilityFunction(kind=\"ei\", kappa=2.5, xi=0.0)\n",
    "\n",
    "        start = time.time()\n",
    "        while time.time() - start < self.timeout:\n",
    "            next_point = optimizer.suggest(utility)\n",
    "            target = objective_function(**next_point)\n",
    "            optimizer.register(params=next_point, target=target)\n",
    "            if verbose:\n",
    "                print(f'Iteration {len(optimizer.space)} - Roll: {next_point[\"roll\"]:.3f}, Pitch: {next_point[\"pitch\"]:.3f}, '\n",
    "                      f'Yaw: {next_point[\"yaw\"]:.3f}, Fitness: {-target:.3f}')\n",
    "\n",
    "        best_params = optimizer.max['params']\n",
    "        best_transformation = self.get_transformation_matrix(best_params['roll'], best_params['pitch'], best_params['yaw'], pcd1_mean, pcd2_mean, T_WC)\n",
    "\n",
    "        reg_p2p = o3d.pipelines.registration.registration_icp(\n",
    "            pcd1_o3d, pcd2_o3d, self.max_correspondence_distance, best_transformation,\n",
    "            o3d.pipelines.registration.TransformationEstimationPointToPoint(),\n",
    "            o3d.pipelines.registration.ICPConvergenceCriteria(max_iteration=self.max_iteration)\n",
    "        )\n",
    "\n",
    "        if visualise_pcds:\n",
    "            self.draw_registration_result(pcd1_o3d, pcd2_o3d, reg_p2p.transformation)\n",
    "\n",
    "        if reg_p2p.fitness == 0:\n",
    "            raise Exception('Open3D ICP was unable to register the two point clouds')\n",
    "\n",
    "        return reg_p2p.transformation\n",
    "\n",
    "    def get_transformation_matrix(self, roll, pitch, yaw, pcd1_mean, pcd2_mean, T_WC):\n",
    "        R = self.euler_angles_to_rotation_matrix([roll, pitch, yaw])\n",
    "        trans_init = np.eye(4)\n",
    "        trans_init[:3, :3] = R\n",
    "        trans_init[:3, 3] = T_WC[:3, :3] @ (pcd2_mean - pcd1_mean + 0.01 * np.random.randn(3))\n",
    "        return trans_init\n",
    "\n",
    "    @staticmethod\n",
    "    def euler_angles_to_rotation_matrix(angles):\n",
    "        roll, pitch, yaw = angles\n",
    "        R_x = np.array([[1, 0, 0],\n",
    "                        [0, np.cos(roll), -np.sin(roll)],\n",
    "                        [0, np.sin(roll), np.cos(roll)]])\n",
    "        R_y = np.array([[np.cos(pitch), 0, np.sin(pitch)],\n",
    "                        [0, 1, 0],\n",
    "                        [-np.sin(pitch), 0, np.cos(pitch)]])\n",
    "        R_z = np.array([[np.cos(yaw), -np.sin(yaw), 0],\n",
    "                        [np.sin(yaw), np.cos(yaw), 0],\n",
    "                        [0, 0, 1]])\n",
    "        return R_z @ R_y @ R_x\n",
    "    @staticmethod\n",
    "    def draw_registration_result(source, target, transformation):\n",
    "        source_temp = source.transform(transformation)\n",
    "        o3d.visualization.draw_geometries([source_temp, target])\n",
    "\n",
    "pcd0 = o3d.geometry.PointCloud()\n",
    "pcd1 = o3d.geometry.PointCloud()\n",
    "\n",
    "pcd0.points = o3d.utility.Vector3dVector(data[\"pc0\"][:, :3])\n",
    "pcd1.points = o3d.utility.Vector3dVector(data[\"pc1\"][:, :3])\n",
    "\n",
    "estimator = Open3dICPPoseEstimator()\n",
    "result = estimator.estimate_relative_pose(pcd0, pcd1, T_WC, verbose=True, visualise_pcds=True)\n",
    "print(\"Best transformation:\")\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 22816 is different from 21767)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 132\u001b[0m\n\u001b[1;32m    129\u001b[0m pcd1\u001b[38;5;241m.\u001b[39mpoints \u001b[38;5;241m=\u001b[39m o3d\u001b[38;5;241m.\u001b[39mutility\u001b[38;5;241m.\u001b[39mVector3dVector(data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpc1\u001b[39m\u001b[38;5;124m\"\u001b[39m][:, :\u001b[38;5;241m3\u001b[39m])\n\u001b[1;32m    131\u001b[0m estimator \u001b[38;5;241m=\u001b[39m Open3dICPPoseEstimator()\n\u001b[0;32m--> 132\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimate_relative_pose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpcd0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpcd1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT_WC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualise_pcds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest transformation:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "Cell \u001b[0;32mIn[37], line 22\u001b[0m, in \u001b[0;36mOpen3dICPPoseEstimator.estimate_relative_pose\u001b[0;34m(self, pcd1_o3d, pcd2_o3d, T_WC, verbose, visualise_pcds)\u001b[0m\n\u001b[1;32m     19\u001b[0m pcd2_mean \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(pcd2_o3d\u001b[38;5;241m.\u001b[39mpoints)\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Initial SVD alignment\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m initial_transform \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msvd_based_initialization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpcd1_o3d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpcd2_o3d\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobjective_function\u001b[39m(roll, pitch, yaw):\n\u001b[1;32m     25\u001b[0m     transformation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_transformation_matrix(roll, pitch, yaw, pcd1_mean, pcd2_mean, initial_transform)\n",
      "Cell \u001b[0;32mIn[37], line 80\u001b[0m, in \u001b[0;36mOpen3dICPPoseEstimator.svd_based_initialization\u001b[0;34m(pcd1_o3d, pcd2_o3d)\u001b[0m\n\u001b[1;32m     77\u001b[0m points1_centered \u001b[38;5;241m=\u001b[39m points1 \u001b[38;5;241m-\u001b[39m centroid1\n\u001b[1;32m     78\u001b[0m points2_centered \u001b[38;5;241m=\u001b[39m points2 \u001b[38;5;241m-\u001b[39m centroid2\n\u001b[0;32m---> 80\u001b[0m H \u001b[38;5;241m=\u001b[39m \u001b[43mpoints1_centered\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpoints2_centered\u001b[49m\n\u001b[1;32m     82\u001b[0m U, S, Vt \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39msvd(H)\n\u001b[1;32m     83\u001b[0m R \u001b[38;5;241m=\u001b[39m Vt\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m U\u001b[38;5;241m.\u001b[39mT\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 22816 is different from 21767)"
     ]
    }
   ],
   "source": [
    "class Open3dICPPoseEstimator:\n",
    "\n",
    "    def __init__(self,\n",
    "                 max_correspondence_distance: float = 0.10,\n",
    "                 max_iteration: int = 10,\n",
    "                 timeout=15):\n",
    "        self.timeout = timeout\n",
    "        self.max_correspondence_distance = max_correspondence_distance\n",
    "        self.max_iteration = max_iteration\n",
    "\n",
    "    def estimate_relative_pose(self,\n",
    "                               pcd1_o3d: o3d.geometry.PointCloud,\n",
    "                               pcd2_o3d: o3d.geometry.PointCloud,\n",
    "                               T_WC: np.ndarray,\n",
    "                               verbose=True,\n",
    "                               visualise_pcds=False) -> np.ndarray:\n",
    "\n",
    "        pcd1_mean = np.asarray(pcd1_o3d.points).mean(axis=0)\n",
    "        pcd2_mean = np.asarray(pcd2_o3d.points).mean(axis=0)\n",
    "\n",
    "        # Initial SVD alignment\n",
    "        initial_transform = self.svd_based_initialization(pcd1_o3d, pcd2_o3d)\n",
    "\n",
    "        def objective_function(roll, pitch, yaw):\n",
    "            transformation = self.get_transformation_matrix(roll, pitch, yaw, pcd1_mean, pcd2_mean, initial_transform)\n",
    "            reg_p2p = o3d.pipelines.registration.registration_icp(\n",
    "                pcd1_o3d, pcd2_o3d, self.max_correspondence_distance, transformation,\n",
    "                o3d.pipelines.registration.TransformationEstimationPointToPoint(),\n",
    "                o3d.pipelines.registration.ICPConvergenceCriteria(max_iteration=self.max_iteration)\n",
    "            )\n",
    "            return -reg_p2p.fitness  # Negate fitness to convert maximization to minimization\n",
    "\n",
    "        pbounds = {'roll': (-np.pi/4, np.pi/4), 'pitch': (-np.pi/4, np.pi/4), 'yaw': (-np.pi/4, np.pi/4)}\n",
    "\n",
    "        optimizer = BayesianOptimization(\n",
    "            f=objective_function,\n",
    "            pbounds=pbounds,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        utility = UtilityFunction(kind=\"ei\", kappa=2.5, xi=0.0)\n",
    "\n",
    "        start = time.time()\n",
    "        while time.time() - start < self.timeout:\n",
    "            next_point = optimizer.suggest(utility)\n",
    "            target = objective_function(**next_point)\n",
    "            optimizer.register(params=next_point, target=target)\n",
    "            if verbose:\n",
    "                print(f'Iteration {len(optimizer.space)} - Roll: {next_point[\"roll\"]:.3f}, Pitch: {next_point[\"pitch\"]:.3f}, '\n",
    "                      f'Yaw: {next_point[\"yaw\"]:.3f}, Fitness: {-target:.3f}')\n",
    "\n",
    "        best_params = optimizer.max['params']\n",
    "        best_transformation = self.get_transformation_matrix(best_params['roll'], best_params['pitch'], best_params['yaw'], pcd1_mean, pcd2_mean, initial_transform)\n",
    "\n",
    "        reg_p2p = o3d.pipelines.registration.registration_icp(\n",
    "            pcd1_o3d, pcd2_o3d, self.max_correspondence_distance, best_transformation,\n",
    "            o3d.pipelines.registration.TransformationEstimationPointToPoint(),\n",
    "            o3d.pipelines.registration.ICPConvergenceCriteria(max_iteration=self.max_iteration)\n",
    "        )\n",
    "\n",
    "        if visualise_pcds:\n",
    "            self.draw_registration_result(pcd1_o3d, pcd2_o3d, reg_p2p.transformation)\n",
    "\n",
    "        if reg_p2p.fitness == 0:\n",
    "            raise Exception('Open3D ICP was unable to register the two point clouds')\n",
    "\n",
    "        return reg_p2p.transformation\n",
    "\n",
    "    def svd_based_initialization(pcd1_o3d, pcd2_o3d):\n",
    "        points1 = np.asarray(pcd1_o3d.points)\n",
    "        points2 = np.asarray(pcd2_o3d.points)\n",
    "\n",
    "        # Ensure the point clouds have the same number of points by random sampling if necessary\n",
    "        if len(points1) != len(points2):\n",
    "            min_points = min(len(points1), len(points2))\n",
    "            points1 = points1[:min_points]\n",
    "            points2 = points2[:min_points]\n",
    "\n",
    "        centroid1 = points1.mean(axis=0)\n",
    "        centroid2 = points2.mean(axis=0)\n",
    "\n",
    "        points1_centered = points1 - centroid1\n",
    "        points2_centered = points2 - centroid2\n",
    "\n",
    "        H = points1_centered.T @ points2_centered\n",
    "\n",
    "        U, S, Vt = np.linalg.svd(H)\n",
    "        R = Vt.T @ U.T\n",
    "\n",
    "        if np.linalg.det(R) < 0:\n",
    "            Vt[-1, :] *= -1\n",
    "            R = Vt.T @ U.T\n",
    "\n",
    "        t = centroid2.T - R @ centroid1.T\n",
    "\n",
    "        transformation = np.eye(4)\n",
    "        transformation[:3, :3] = R\n",
    "        transformation[:3, 3] = t\n",
    "\n",
    "        return transformation\n",
    "\n",
    "    def get_transformation_matrix(self, roll, pitch, yaw, pcd1_mean, pcd2_mean, initial_transform):\n",
    "        R = self.euler_angles_to_rotation_matrix([roll, pitch, yaw])\n",
    "        trans_init = np.eye(4)\n",
    "        trans_init[:3, :3] = R @ initial_transform[:3, :3]\n",
    "        trans_init[:3, 3] = initial_transform[:3, 3] + 0.01 * np.random.randn(3)\n",
    "        return trans_init\n",
    "\n",
    "    @staticmethod\n",
    "    def euler_angles_to_rotation_matrix(angles):\n",
    "        roll, pitch, yaw = angles\n",
    "        R_x = np.array([[1, 0, 0],\n",
    "                        [0, np.cos(roll), -np.sin(roll)],\n",
    "                        [0, np.sin(roll), np.cos(roll)]])\n",
    "        R_y = np.array([[np.cos(pitch), 0, np.sin(pitch)],\n",
    "                        [0, 1, 0],\n",
    "                        [-np.sin(pitch), 0, np.cos(pitch)]])\n",
    "        R_z = np.array([[np.cos(yaw), -np.sin(yaw), 0],\n",
    "                        [np.sin(yaw), np.cos(yaw), 0],\n",
    "                        [0, 0, 1]])\n",
    "        return R_z @ R_y @ R_x\n",
    "\n",
    "    @staticmethod\n",
    "    def draw_registration_result(source, target, transformation):\n",
    "        source_temp = source.transform(transformation)\n",
    "        o3d.visualization.draw_geometries([source_temp, target])\n",
    "\n",
    "# Example usage\n",
    "# data[\"pc0\"] and data[\"pc1\"] are your point cloud data arrays.\n",
    "pcd0 = o3d.geometry.PointCloud()\n",
    "pcd1 = o3d.geometry.PointCloud()\n",
    "\n",
    "pcd0.points = o3d.utility.Vector3dVector(data[\"pc0\"][:, :3])\n",
    "pcd1.points = o3d.utility.Vector3dVector(data[\"pc1\"][:, :3])\n",
    "\n",
    "estimator = Open3dICPPoseEstimator()\n",
    "result = estimator.estimate_relative_pose(pcd0, pcd1, T_WC, verbose=True, visualise_pcds=True)\n",
    "print(\"Best transformation:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.00000000e+00,  5.76610702e-18,  4.24064341e-18,\n",
       "        -5.48792420e-02],\n",
       "       [ 5.76610702e-18,  1.00000000e+00,  3.26733594e-17,\n",
       "         1.82634440e-01],\n",
       "       [ 4.24064341e-18,  3.26733594e-17,  1.00000000e+00,\n",
       "         6.19532222e-05],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "def create_homogeneous_matrix(xyz, quaternion):\n",
    "    # Convert the quaternion to a rotation matrix\n",
    "    rotation_matrix = R.from_quat(quaternion).as_matrix()\n",
    "    # Create a homogeneous transformation matrix\n",
    "    T = np.eye(4)  # Start with an identity matrix\n",
    "    T[:3, :3] = rotation_matrix  # Insert the rotation matrix\n",
    "    T[:3, 3] = xyz  # Insert the translation vector\n",
    "\n",
    "    return T\n",
    "\n",
    "simple_T = create_homogeneous_matrix(diff, [0, 0, 0, 1])\n",
    "T_WC = np.load(\"../handeye/T_WC_head.npy\")\n",
    "T_delta_world = T_WC @ simple_T @ pose_inv(T_WC)\n",
    "T_delta_world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.920013189315796\n",
      "[[ 0.85319111 -0.51953292 -0.04637329  0.1287207 ]\n",
      " [ 0.51371596  0.85236793 -0.09779997 -0.2560663 ]\n",
      " [ 0.09033741  0.05961937  0.99412508 -0.04125192]\n",
      " [ 0.          0.          0.          1.        ]]\n"
     ]
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "import copy\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "# Assuming data[\"pc0\"] and data[\"pc1\"] are your point cloud data\n",
    "pcd0 = o3d.geometry.PointCloud()\n",
    "pcd1 = o3d.geometry.PointCloud()\n",
    "\n",
    "pcd0.points = o3d.utility.Vector3dVector(data[\"pc0\"][:, :3])\n",
    "pcd1.points = o3d.utility.Vector3dVector(data[\"pc1\"][:, :3])\n",
    "\n",
    "# # Estimate normals for each point cloud\n",
    "# pcd0.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=0.1, max_nn=40))\n",
    "# pcd1.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=0.1, max_nn=40))\n",
    "\n",
    "# Function to draw registration results\n",
    "def draw_registration_result(source, target, transformation):\n",
    "    source_temp = copy.deepcopy(source)\n",
    "    target_temp = copy.deepcopy(target)\n",
    "    source_temp.transform(transformation)\n",
    "    source_temp.paint_uniform_color([1, 0.706, 0])\n",
    "    target_temp.paint_uniform_color([0, 0.651, 0.929])\n",
    "    o3d.visualization.draw_geometries([source_temp, target_temp])\n",
    "\n",
    "# Compute FPFH features\n",
    "voxel_size = 0.05  # Set voxel size for downsampling (adjust based on your data)\n",
    "source_down = pcd0.voxel_down_sample(voxel_size)\n",
    "target_down = pcd1.voxel_down_sample(voxel_size)\n",
    "\n",
    "source_down.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=voxel_size * 2, max_nn=30))\n",
    "target_down.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=voxel_size * 2, max_nn=30))\n",
    "\n",
    "source_fpfh = o3d.pipelines.registration.compute_fpfh_feature(\n",
    "    source_down,\n",
    "    o3d.geometry.KDTreeSearchParamHybrid(radius=voxel_size * 5, max_nn=100))\n",
    "\n",
    "target_fpfh = o3d.pipelines.registration.compute_fpfh_feature(\n",
    "    target_down,\n",
    "    o3d.geometry.KDTreeSearchParamHybrid(radius=voxel_size * 5, max_nn=100))\n",
    "\n",
    "# Global registration using RANSAC\n",
    "distance_threshold = voxel_size * 1.5\n",
    "result = o3d.pipelines.registration.registration_ransac_based_on_feature_matching(\n",
    "    source_down, target_down, source_fpfh, target_fpfh, mutual_filter=False,\n",
    "    max_correspondence_distance=distance_threshold,\n",
    "    estimation_method=o3d.pipelines.registration.TransformationEstimationPointToPoint(), \n",
    "    ransac_n=4,\n",
    "    checkers=[\n",
    "        o3d.pipelines.registration.CorrespondenceCheckerBasedOnEdgeLength(0.9), \n",
    "        o3d.pipelines.registration.CorrespondenceCheckerBasedOnDistance(distance_threshold)\n",
    "    ],\n",
    "    criteria=o3d.pipelines.registration.RANSACConvergenceCriteria(4000000, 500)\n",
    ")\n",
    "\n",
    "# Use the result of global registration as the initial transformation for ICP\n",
    "trans_init = result.transformation\n",
    "# Apply ICP\n",
    "threshold = 0.01  # Set a threshold for ICP, this depends on your data\n",
    "reg_p2p = o3d.pipelines.registration.registration_icp(\n",
    "    pcd0, pcd1, threshold, trans_init,\n",
    "    o3d.pipelines.registration.TransformationEstimationPointToPoint())\n",
    "\n",
    "# Get the transformation matrix\n",
    "T_delta_cam = reg_p2p.transformation\n",
    "\n",
    "print(time.time() - start)\n",
    "# Draw the result\n",
    "draw_registration_result(pcd0, pcd1, T_delta_cam)\n",
    "\n",
    "print(T_delta_cam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: Apply fast global registration with distance threshold 0.003\n",
      "0.17958688735961914\n",
      "[[ 0.9623048   0.12707494 -0.24046086 -0.02170186]\n",
      " [-0.12790808  0.99171092  0.01220592  0.0214484 ]\n",
      " [ 0.24001873  0.01901107  0.97058209 -0.0313623 ]\n",
      " [ 0.          0.          0.          1.        ]]\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "# Assuming data[\"pc0\"] and data[\"pc1\"] are your point cloud data\n",
    "pcd0 = o3d.geometry.PointCloud()\n",
    "pcd1 = o3d.geometry.PointCloud()\n",
    "\n",
    "pcd0.points = o3d.utility.Vector3dVector(data[\"pc0\"][:, :3])\n",
    "pcd1.points = o3d.utility.Vector3dVector(data[\"pc1\"][:, :3])\n",
    "\n",
    "# Estimate normals for each point cloud\n",
    "# pcd0.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=0.1, max_nn=40))\n",
    "# pcd1.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=0.1, max_nn=40))\n",
    "\n",
    "# Function to draw registration results\n",
    "def draw_registration_result(source, target, transformation):\n",
    "    source_temp = copy.deepcopy(source)\n",
    "    target_temp = copy.deepcopy(target)\n",
    "    source_temp.transform(transformation)\n",
    "    source_temp.paint_uniform_color([1, 0.706, 0])\n",
    "    target_temp.paint_uniform_color([0, 0.651, 0.929])\n",
    "    o3d.visualization.draw_geometries([source_temp, target_temp])\n",
    "\n",
    "# Compute FPFH features\n",
    "voxel_size = 0.005  # Set voxel size for downsampling (adjust based on your data)\n",
    "source_down = pcd0.voxel_down_sample(voxel_size)\n",
    "target_down = pcd1.voxel_down_sample(voxel_size)\n",
    "\n",
    "source_down.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=voxel_size * 2, max_nn=30))\n",
    "target_down.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=voxel_size * 2, max_nn=30))\n",
    "\n",
    "source_fpfh = o3d.pipelines.registration.compute_fpfh_feature(\n",
    "    source_down,\n",
    "    o3d.geometry.KDTreeSearchParamHybrid(radius=voxel_size * 5, max_nn=100))\n",
    "\n",
    "target_fpfh = o3d.pipelines.registration.compute_fpfh_feature(\n",
    "    target_down,\n",
    "    o3d.geometry.KDTreeSearchParamHybrid(radius=voxel_size * 5, max_nn=100))\n",
    "\n",
    "def execute_fast_global_registration(source_down, target_down, source_fpfh,\n",
    "                                     target_fpfh, voxel_size):\n",
    "    distance_threshold = voxel_size * 0.5\n",
    "    print(\":: Apply fast global registration with distance threshold %.3f\" \\\n",
    "            % distance_threshold)\n",
    "    result = o3d.pipelines.registration.registration_fgr_based_on_feature_matching(\n",
    "        source_down, target_down, source_fpfh, target_fpfh,\n",
    "        o3d.pipelines.registration.FastGlobalRegistrationOption(\n",
    "            maximum_correspondence_distance=distance_threshold))\n",
    "    return result\n",
    "\n",
    "result_fast = execute_fast_global_registration(source_down, target_down,\n",
    "                                               source_fpfh, target_fpfh,\n",
    "                                               voxel_size)\n",
    "\n",
    "# Use the result of global registration as the initial transformation for ICP\n",
    "trans_init = result_fast.transformation\n",
    "# Apply ICP\n",
    "threshold = 0.01  # Set a threshold for ICP, this depends on your data\n",
    "reg_p2p = o3d.pipelines.registration.registration_icp(\n",
    "    pcd0, pcd1, threshold, trans_init,\n",
    "    o3d.pipelines.registration.TransformationEstimationPointToPoint())\n",
    "\n",
    "# Get the transformation matrix\n",
    "T_delta_cam = reg_p2p.transformation\n",
    "\n",
    "\n",
    "print(time.time() - start)\n",
    "# Draw the result\n",
    "draw_registration_result(pcd0, pcd1, T_delta_cam)\n",
    "\n",
    "print(T_delta_cam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.96091092, -0.27669804, -0.00940215,  0.06690749],\n",
       "       [ 0.27684587,  0.96062984,  0.02338089, -0.31354479],\n",
       "       [ 0.00256254, -0.0250699 ,  0.99968242, -0.0022233 ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  1.        ]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T_WC = np.load(\"../handeye/T_WC_head.npy\")\n",
    "T_delta_world = T_WC @ T_delta_cam @ pose_inv(T_WC)\n",
    "T_delta_world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.96091408, -0.27684678,  0.        ],\n",
       "       [ 0.27684678,  0.96091408,  0.        ],\n",
       "       [ 0.        ,  0.        ,  1.        ]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = R.from_matrix(T_delta_world[:3, :3]).as_euler(\"xyz\")\n",
    "yaw_only_delta_rotation = R.from_euler(\"xyz\", [0, 0, r[-1]]).as_matrix()\n",
    "yaw_only_delta_rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'T_delta_world' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m     rotation \u001b[38;5;241m=\u001b[39m R\u001b[38;5;241m.\u001b[39mfrom_matrix(rotation_matrix)\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rotation\u001b[38;5;241m.\u001b[39mas_euler(seq\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXYZ\u001b[39m\u001b[38;5;124m\"\u001b[39m, degrees\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 14\u001b[0m trans \u001b[38;5;241m=\u001b[39m translation_from_matrix(\u001b[43mT_delta_world\u001b[49m)\n\u001b[1;32m     15\u001b[0m rotation \u001b[38;5;241m=\u001b[39m euler_from_matrix(T_delta_world)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(trans, rotation)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'T_delta_world' is not defined"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "\n",
    "def translation_from_matrix(matrix):\n",
    "    \"\"\"Extracts the translation vector from a 4x4 homogeneous transformation matrix.\"\"\"\n",
    "    return matrix[:3, 3]\n",
    "\n",
    "def euler_from_matrix(matrix):\n",
    "    \"\"\"Extracts the quaternion from a 4x4 homogeneous transformation matrix.\"\"\"\n",
    "    rotation_matrix = matrix[:3, :3].copy()\n",
    "    rotation = R.from_matrix(rotation_matrix)\n",
    "    return rotation.as_euler(seq=\"XYZ\", degrees=True)\n",
    "\n",
    "trans = translation_from_matrix(T_delta_world)\n",
    "rotation = euler_from_matrix(T_delta_world)\n",
    "\n",
    "print(trans, rotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.99361179, -0.02611172,  0.10978974,  0.50418126],\n",
       "       [-0.02759997, -0.99954633,  0.01205746,  0.09185436],\n",
       "       [ 0.10942509, -0.01501063, -0.9938817 ,  0.48143709],\n",
       "       [ 0.        ,  0.        ,  0.        ,  1.        ]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_homogeneous_matrix(xyz, quaternion):\n",
    "    # Convert the quaternion to a rotation matrix\n",
    "    rotation_matrix = R.from_quat(quaternion).as_matrix()\n",
    "    # Create a homogeneous transformation matrix\n",
    "    T = np.eye(4)  # Start with an identity matrix\n",
    "    T[:3, :3] = rotation_matrix  # Insert the rotation matrix\n",
    "    T[:3, 3] = xyz  # Insert the translation vector\n",
    "\n",
    "    return T\n",
    "\n",
    "demo = [\n",
    "    0.5041812568964179,\n",
    "    0.09185436015924689,\n",
    "    0.48143709451847916,\n",
    "    -0.9983786626178943,\n",
    "    0.013449729176136651,\n",
    "    -0.054892707858185244,\n",
    "    0.006778011389003352\n",
    "]\n",
    "\n",
    "T_eef = create_homogeneous_matrix(demo[:3], demo[3:])\n",
    "T_eef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.80887007,  0.58083391,  0.09143946,  0.62268341],\n",
       "       [ 0.57427787, -0.81378487,  0.08921382, -0.0129589 ],\n",
       "       [ 0.12623046, -0.01965073, -0.99180629,  0.48391332],\n",
       "       [ 0.        ,  0.        ,  0.        ,  1.        ]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T_delta_world @ T_eef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.74345614, -0.66244621,  0.09185851,  0.62268341],\n",
       "       [-0.65977202, -0.74895908, -0.06132849, -0.0129589 ],\n",
       "       [ 0.10942509, -0.01501063, -0.9938817 ,  0.48391332],\n",
       "       [ 0.        ,  0.        ,  0.        ,  1.        ]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_T @ T_eef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.76608952,  0.64273389,  0.        ,  0.28067213],\n",
       "       [-0.64273389,  0.76608952, -0.        , -0.38310653],\n",
       "       [-0.        ,  0.        ,  1.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  1.        ]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T_bias = create_homogeneous_matrix([trans[0], trans[1], 0], R.from_euler('xyz', [0, 0, rotation[2]]).as_quat())\n",
    "T_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.12649077, -0.10964465, -0.02777028])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R.from_matrix(T_eef[:3, :3]).as_euler('xyz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.12649077, -0.10964465, -0.72583185])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = T_bias @ T_eef\n",
    "R.from_matrix(res[:3, :3]).as_euler('xyz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.74345614, -0.66244621,  0.09185851],\n",
       "       [-0.65977202, -0.74895908, -0.06132849],\n",
       "       [ 0.10942509, -0.01501063, -0.9938817 ]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R.from_euler('xyz', [0, 0, rotation[2]]).as_matrix() @ T_eef[:3, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pose_inv(pose):\n",
    "    \"\"\"Inverse a 4x4 homogeneous transformation matrix.\"\"\"\n",
    "    R = pose[:3, :3]\n",
    "    T = np.eye(4)\n",
    "    T[:3, :3] = R.T\n",
    "    T[:3, 3] = - R.T @ np.ascontiguousarray(pose[:3, 3])\n",
    "    return T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8.04888450e-01, -6.66575509e-17,  5.93426139e-01],\n",
       "       [-4.17340537e-01,  7.10919961e-01,  5.66056256e-01],\n",
       "       [-4.21878488e-01, -7.03272926e-01,  5.72211266e-01]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "r_project = np.linalg.inv(R.from_euler('xyz', [rotation[0], rotation[1], rotation[2]]).as_matrix()) @ R.from_euler('xyz', [0, 0, rotation[2]]).as_matrix()\n",
    "r_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.76608952,  0.64273389,  0.        ],\n",
       "       [-0.64273389,  0.76608952, -0.        ],\n",
       "       [-0.        ,  0.        ,  1.        ]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = R.from_euler('xyz', [0, 0, rotation[-1]])\n",
    "\n",
    "# Get the rotation matrix\n",
    "adjusted_rotation_matrix = r.as_matrix()\n",
    "\n",
    "adjusted_rotation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.17739752, 0.24072682, 0.00247622])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjusted_translation = T_delta_world[:3, :3] @ T_eef[:3, 3] - adjusted_rotation_matrix @ T_eef[:3, 3] + T_delta_world[:3, 3]\n",
    "adjusted_translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.76608952,  0.64273389,  0.        ,  0.17739752],\n",
       "       [-0.64273389,  0.76608952, -0.        ,  0.24072682],\n",
       "       [-0.        ,  0.        ,  1.        ,  0.00247622],\n",
       "       [ 0.        ,  0.        ,  0.        ,  1.        ]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_T =  create_homogeneous_matrix(adjusted_translation, r.as_quat())\n",
    "new_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.79857538, -0.60179267, -0.011088  ,  0.28067213],\n",
       "       [ 0.60165335,  0.79864132, -0.01361215, -0.38310653],\n",
       "       [ 0.01704703,  0.00419919,  0.99984587, -0.00643008],\n",
       "       [ 0.        ,  0.        ,  0.        ,  1.        ]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T_delta_world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted rotation -1854.694161068739\n",
      "[-0.09844735 -0.39142355 -0.21084177]\n",
      "[[ 0.57794079  0.8160787   0.         -0.09844735]\n",
      " [-0.8160787   0.57794079  0.         -0.39142355]\n",
      " [ 0.          0.          1.         -0.21084177]\n",
      " [ 0.          0.          0.          1.        ]]\n"
     ]
    }
   ],
   "source": [
    "PointCloud = np.ndarray\n",
    "\n",
    "def rotate_pointcloud(pcd: PointCloud, angle_z: float):\n",
    "    print(\"predicted rotation\", np.rad2deg(angle_z))\n",
    "    R = np.eye(3)\n",
    "    cosine = np.cos(angle_z)\n",
    "    sine = np.sin(angle_z)\n",
    "    R[0, 0] = cosine\n",
    "    R[1, 1] = cosine\n",
    "    R[0, 1] = -sine\n",
    "    R[1, 0] = sine\n",
    "\n",
    "    pcd[:3, :] = R @ pcd[:3, :]\n",
    "    return R, pcd\n",
    "\n",
    "def find_translation(pcd0: PointCloud, pcd1: PointCloud) -> np.ndarray:\n",
    "    pcd0_centre = np.mean(pcd0[:3, :], axis=1)\n",
    "    pcd1_centre = np.mean(pcd1[:3, :], axis=1)\n",
    "    return pcd1_centre - pcd0_centre\n",
    "    \n",
    "R_mtx, rotated_pcd0 = rotate_pointcloud(data[\"pc0\"], rotation[-1])\n",
    "translation = find_translation(rotated_pcd0, data[\"pc1\"])\n",
    "\n",
    "print(translation)\n",
    "T_delta_base = np.eye(4)\n",
    "T_delta_base[:3, :3] = R_mtx\n",
    "T_delta_base[:3, 3] = translation\n",
    "\n",
    "T_delta_cam = pose_inv(data[\"T_WC\"]) @ T_delta_base @ data[\"T_WC\"]\n",
    "\n",
    "print(T_delta_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "\n",
    "width = 848  # Replace with your camera image width\n",
    "height = 480 # Replace with your camera image height\n",
    "fx = 431.56503296\n",
    "fy = 431.18637085\n",
    "cx = 418.71490479\n",
    "cy = 235.15617371 \n",
    "intrinsics = o3d.camera.PinholeCameraIntrinsic(width, height, fx, fy, cx, cy)\n",
    "\n",
    "demo_mask = np.array(Image.open(\"../data/lego/demo_wrist_mask.png\"))\n",
    "demo_rgb = np.array(Image.open(\"../data/lego/demo_wrist_rgb.png\")) \n",
    "demo_depth = np.array(Image.open(\"../data/lego/demo_wrist_depth.png\")).astype(np.uint16)\n",
    "\n",
    "color = o3d.geometry.Image(demo_rgb)\n",
    "depth = o3d.geometry.Image(demo_depth)\n",
    "\n",
    "rgbd_image = o3d.geometry.RGBDImage.create_from_color_and_depth(\n",
    "    color, depth, depth_scale=1000.0, convert_rgb_to_intensity=False)\n",
    "\n",
    "rgbd_image\n",
    "pcd = o3d.geometry.PointCloud.create_from_rgbd_image(rgbd_image, intrinsics) \n",
    "\n",
    "# Visualization\n",
    "o3d.visualization.draw_geometries([pcd]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
