{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from direct.preprocessor import Preprocessor, pose_inv, SceneData\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "\n",
    "dir = \"experiments/scissor\"\n",
    "\n",
    "demo_head_rgb = np.array(Image.open(\"../{0}/demo_head_rgb.png\".format(dir)))\n",
    "demo_head_depth = np.array(Image.open(\"../{0}/demo_head_depth.png\".format(dir)))\n",
    "demo_head_mask = np.array(Image.open(\"../{0}/demo_head_seg.png\".format(dir)))\n",
    "\n",
    "live_head_rgb = np.array(Image.open(\"../{0}/live_d415_rgb.png\".format(dir)))\n",
    "live_head_depth = np.array(Image.open(\"../{0}/live_d415_depth.png\".format(dir)))\n",
    "live_head_mask = np.array(Image.open(\"../{0}/live_d415_seg.png\".format(dir)))\n",
    "\n",
    "intrinsics_d415 = np.load(\"../handeye/intrinsics_d415.npy\")\n",
    "intrinsics_d405 = np.load(\"../handeye/intrinsics_d405.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "data = SceneData(\n",
    "    image_0=demo_head_rgb,\n",
    "    image_1=live_head_rgb,\n",
    "    depth_0=demo_head_depth,\n",
    "    depth_1=live_head_depth,\n",
    "    seg_0=demo_head_mask,\n",
    "    seg_1=live_head_mask,\n",
    "    intrinsics_0=intrinsics_d415,\n",
    "    intrinsics_1=intrinsics_d415,\n",
    "    T_WC=np.eye(4) # cam frame\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.0556434e-05 9.6965443e-05 7.0472446e-04]\n",
      "[7.1257386e-06 8.1077611e-05 7.2817306e-04]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.3430696e-05,  1.5887832e-05, -2.3448607e-05], dtype=float32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor = Preprocessor()\n",
    "data.update(processor(data))\n",
    "\n",
    "pcd0 = o3d.geometry.PointCloud()\n",
    "pcd0.points = o3d.utility.Vector3dVector(data[\"pc0\"][:, :3])\n",
    "pcd1 = o3d.geometry.PointCloud()\n",
    "pcd1.points = o3d.utility.Vector3dVector(data[\"pc1\"][:, :3])\n",
    "o3d.visualization.draw_geometries([pcd0, pcd1])\n",
    "\n",
    "# Calculate the centroid of each point cloud\n",
    "pcd0_centre = np.mean(data[\"pc0\"][:, :3], axis=0)  # Calculate mean across columns (axis=0)\n",
    "pcd1_centre = np.mean(data[\"pc1\"][:, :3], axis=0)  # Calculate mean across columns (axis=0)\n",
    "\n",
    "print(pcd0_centre)\n",
    "print(pcd1_centre)\n",
    "# # Compute the difference between the centroids\n",
    "diff = pcd0_centre - pcd1_centre\n",
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.00000000e+00,  5.76610702e-18,  4.24064341e-18,\n",
       "        -5.48792420e-02],\n",
       "       [ 5.76610702e-18,  1.00000000e+00,  3.26733594e-17,\n",
       "         1.82634440e-01],\n",
       "       [ 4.24064341e-18,  3.26733594e-17,  1.00000000e+00,\n",
       "         6.19532222e-05],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "def create_homogeneous_matrix(xyz, quaternion):\n",
    "    # Convert the quaternion to a rotation matrix\n",
    "    rotation_matrix = R.from_quat(quaternion).as_matrix()\n",
    "    # Create a homogeneous transformation matrix\n",
    "    T = np.eye(4)  # Start with an identity matrix\n",
    "    T[:3, :3] = rotation_matrix  # Insert the rotation matrix\n",
    "    T[:3, 3] = xyz  # Insert the translation vector\n",
    "\n",
    "    return T\n",
    "\n",
    "simple_T = create_homogeneous_matrix(diff, [0, 0, 0, 1])\n",
    "T_WC = np.load(\"../handeye/T_WC_head.npy\")\n",
    "T_delta_world = T_WC @ simple_T @ pose_inv(T_WC)\n",
    "T_delta_world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.737471103668213\n",
      "[[ 0.96061271  0.13025186 -0.2454744  -0.01796316]\n",
      " [-0.12988129  0.99137029  0.01777055  0.0172543 ]\n",
      " [ 0.24567067  0.01481192  0.96924018 -0.02960126]\n",
      " [ 0.          0.          0.          1.        ]]\n"
     ]
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "import copy\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "# Assuming data[\"pc0\"] and data[\"pc1\"] are your point cloud data\n",
    "pcd0 = o3d.geometry.PointCloud()\n",
    "pcd1 = o3d.geometry.PointCloud()\n",
    "\n",
    "pcd0.points = o3d.utility.Vector3dVector(data[\"pc0\"][:, :3])\n",
    "pcd1.points = o3d.utility.Vector3dVector(data[\"pc1\"][:, :3])\n",
    "\n",
    "# # Estimate normals for each point cloud\n",
    "# pcd0.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=0.1, max_nn=40))\n",
    "# pcd1.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=0.1, max_nn=40))\n",
    "\n",
    "# Function to draw registration results\n",
    "def draw_registration_result(source, target, transformation):\n",
    "    source_temp = copy.deepcopy(source)\n",
    "    target_temp = copy.deepcopy(target)\n",
    "    source_temp.transform(transformation)\n",
    "    source_temp.paint_uniform_color([1, 0.706, 0])\n",
    "    target_temp.paint_uniform_color([0, 0.651, 0.929])\n",
    "    o3d.visualization.draw_geometries([source_temp, target_temp])\n",
    "\n",
    "# Compute FPFH features\n",
    "voxel_size = 0.05  # Set voxel size for downsampling (adjust based on your data)\n",
    "source_down = pcd0.voxel_down_sample(voxel_size)\n",
    "target_down = pcd1.voxel_down_sample(voxel_size)\n",
    "\n",
    "source_down.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=voxel_size * 2, max_nn=30))\n",
    "target_down.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=voxel_size * 2, max_nn=30))\n",
    "\n",
    "source_fpfh = o3d.pipelines.registration.compute_fpfh_feature(\n",
    "    source_down,\n",
    "    o3d.geometry.KDTreeSearchParamHybrid(radius=voxel_size * 5, max_nn=100))\n",
    "\n",
    "target_fpfh = o3d.pipelines.registration.compute_fpfh_feature(\n",
    "    target_down,\n",
    "    o3d.geometry.KDTreeSearchParamHybrid(radius=voxel_size * 5, max_nn=100))\n",
    "\n",
    "# Global registration using RANSAC\n",
    "distance_threshold = voxel_size * 1.5\n",
    "result = o3d.pipelines.registration.registration_ransac_based_on_feature_matching(\n",
    "    source_down, target_down, source_fpfh, target_fpfh, mutual_filter=False,\n",
    "    max_correspondence_distance=distance_threshold,\n",
    "    estimation_method=o3d.pipelines.registration.TransformationEstimationPointToPoint(), \n",
    "    ransac_n=4,\n",
    "    checkers=[\n",
    "        o3d.pipelines.registration.CorrespondenceCheckerBasedOnEdgeLength(0.9), \n",
    "        o3d.pipelines.registration.CorrespondenceCheckerBasedOnDistance(distance_threshold)\n",
    "    ],\n",
    "    criteria=o3d.pipelines.registration.RANSACConvergenceCriteria(4000000, 500)\n",
    ")\n",
    "\n",
    "# Use the result of global registration as the initial transformation for ICP\n",
    "trans_init = result.transformation\n",
    "# Apply ICP\n",
    "threshold = 0.01  # Set a threshold for ICP, this depends on your data\n",
    "reg_p2p = o3d.pipelines.registration.registration_icp(\n",
    "    pcd0, pcd1, threshold, trans_init,\n",
    "    o3d.pipelines.registration.TransformationEstimationPointToPoint())\n",
    "\n",
    "# Get the transformation matrix\n",
    "T_delta_cam = reg_p2p.transformation\n",
    "\n",
    "print(time.time() - start)\n",
    "# Draw the result\n",
    "draw_registration_result(pcd0, pcd1, T_delta_cam)\n",
    "\n",
    "print(T_delta_cam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: Apply fast global registration with distance threshold 0.003\n",
      "0.17958688735961914\n",
      "[[ 0.9623048   0.12707494 -0.24046086 -0.02170186]\n",
      " [-0.12790808  0.99171092  0.01220592  0.0214484 ]\n",
      " [ 0.24001873  0.01901107  0.97058209 -0.0313623 ]\n",
      " [ 0.          0.          0.          1.        ]]\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "# Assuming data[\"pc0\"] and data[\"pc1\"] are your point cloud data\n",
    "pcd0 = o3d.geometry.PointCloud()\n",
    "pcd1 = o3d.geometry.PointCloud()\n",
    "\n",
    "pcd0.points = o3d.utility.Vector3dVector(data[\"pc0\"][:, :3])\n",
    "pcd1.points = o3d.utility.Vector3dVector(data[\"pc1\"][:, :3])\n",
    "\n",
    "# Estimate normals for each point cloud\n",
    "# pcd0.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=0.1, max_nn=40))\n",
    "# pcd1.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=0.1, max_nn=40))\n",
    "\n",
    "# Function to draw registration results\n",
    "def draw_registration_result(source, target, transformation):\n",
    "    source_temp = copy.deepcopy(source)\n",
    "    target_temp = copy.deepcopy(target)\n",
    "    source_temp.transform(transformation)\n",
    "    source_temp.paint_uniform_color([1, 0.706, 0])\n",
    "    target_temp.paint_uniform_color([0, 0.651, 0.929])\n",
    "    o3d.visualization.draw_geometries([source_temp, target_temp])\n",
    "\n",
    "# Compute FPFH features\n",
    "voxel_size = 0.005  # Set voxel size for downsampling (adjust based on your data)\n",
    "source_down = pcd0.voxel_down_sample(voxel_size)\n",
    "target_down = pcd1.voxel_down_sample(voxel_size)\n",
    "\n",
    "source_down.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=voxel_size * 2, max_nn=30))\n",
    "target_down.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=voxel_size * 2, max_nn=30))\n",
    "\n",
    "source_fpfh = o3d.pipelines.registration.compute_fpfh_feature(\n",
    "    source_down,\n",
    "    o3d.geometry.KDTreeSearchParamHybrid(radius=voxel_size * 5, max_nn=100))\n",
    "\n",
    "target_fpfh = o3d.pipelines.registration.compute_fpfh_feature(\n",
    "    target_down,\n",
    "    o3d.geometry.KDTreeSearchParamHybrid(radius=voxel_size * 5, max_nn=100))\n",
    "\n",
    "def execute_fast_global_registration(source_down, target_down, source_fpfh,\n",
    "                                     target_fpfh, voxel_size):\n",
    "    distance_threshold = voxel_size * 0.5\n",
    "    print(\":: Apply fast global registration with distance threshold %.3f\" \\\n",
    "            % distance_threshold)\n",
    "    result = o3d.pipelines.registration.registration_fgr_based_on_feature_matching(\n",
    "        source_down, target_down, source_fpfh, target_fpfh,\n",
    "        o3d.pipelines.registration.FastGlobalRegistrationOption(\n",
    "            maximum_correspondence_distance=distance_threshold))\n",
    "    return result\n",
    "\n",
    "result_fast = execute_fast_global_registration(source_down, target_down,\n",
    "                                               source_fpfh, target_fpfh,\n",
    "                                               voxel_size)\n",
    "\n",
    "# Use the result of global registration as the initial transformation for ICP\n",
    "trans_init = result_fast.transformation\n",
    "# Apply ICP\n",
    "threshold = 0.01  # Set a threshold for ICP, this depends on your data\n",
    "reg_p2p = o3d.pipelines.registration.registration_icp(\n",
    "    pcd0, pcd1, threshold, trans_init,\n",
    "    o3d.pipelines.registration.TransformationEstimationPointToPoint())\n",
    "\n",
    "# Get the transformation matrix\n",
    "T_delta_cam = reg_p2p.transformation\n",
    "\n",
    "\n",
    "print(time.time() - start)\n",
    "# Draw the result\n",
    "draw_registration_result(pcd0, pcd1, T_delta_cam)\n",
    "\n",
    "print(T_delta_cam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.96091092, -0.27669804, -0.00940215,  0.06690749],\n",
       "       [ 0.27684587,  0.96062984,  0.02338089, -0.31354479],\n",
       "       [ 0.00256254, -0.0250699 ,  0.99968242, -0.0022233 ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  1.        ]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T_WC = np.load(\"../handeye/T_WC_head.npy\")\n",
    "T_delta_world = T_WC @ T_delta_cam @ pose_inv(T_WC)\n",
    "T_delta_world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.96091408, -0.27684678,  0.        ],\n",
       "       [ 0.27684678,  0.96091408,  0.        ],\n",
       "       [ 0.        ,  0.        ,  1.        ]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = R.from_matrix(T_delta_world[:3, :3]).as_euler(\"xyz\")\n",
    "yaw_only_delta_rotation = R.from_euler(\"xyz\", [0, 0, r[-1]]).as_matrix()\n",
    "yaw_only_delta_rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'T_delta_world' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m     rotation \u001b[38;5;241m=\u001b[39m R\u001b[38;5;241m.\u001b[39mfrom_matrix(rotation_matrix)\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rotation\u001b[38;5;241m.\u001b[39mas_euler(seq\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXYZ\u001b[39m\u001b[38;5;124m\"\u001b[39m, degrees\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 14\u001b[0m trans \u001b[38;5;241m=\u001b[39m translation_from_matrix(\u001b[43mT_delta_world\u001b[49m)\n\u001b[1;32m     15\u001b[0m rotation \u001b[38;5;241m=\u001b[39m euler_from_matrix(T_delta_world)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(trans, rotation)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'T_delta_world' is not defined"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "\n",
    "def translation_from_matrix(matrix):\n",
    "    \"\"\"Extracts the translation vector from a 4x4 homogeneous transformation matrix.\"\"\"\n",
    "    return matrix[:3, 3]\n",
    "\n",
    "def euler_from_matrix(matrix):\n",
    "    \"\"\"Extracts the quaternion from a 4x4 homogeneous transformation matrix.\"\"\"\n",
    "    rotation_matrix = matrix[:3, :3].copy()\n",
    "    rotation = R.from_matrix(rotation_matrix)\n",
    "    return rotation.as_euler(seq=\"XYZ\", degrees=True)\n",
    "\n",
    "trans = translation_from_matrix(T_delta_world)\n",
    "rotation = euler_from_matrix(T_delta_world)\n",
    "\n",
    "print(trans, rotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.99361179, -0.02611172,  0.10978974,  0.50418126],\n",
       "       [-0.02759997, -0.99954633,  0.01205746,  0.09185436],\n",
       "       [ 0.10942509, -0.01501063, -0.9938817 ,  0.48143709],\n",
       "       [ 0.        ,  0.        ,  0.        ,  1.        ]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_homogeneous_matrix(xyz, quaternion):\n",
    "    # Convert the quaternion to a rotation matrix\n",
    "    rotation_matrix = R.from_quat(quaternion).as_matrix()\n",
    "    # Create a homogeneous transformation matrix\n",
    "    T = np.eye(4)  # Start with an identity matrix\n",
    "    T[:3, :3] = rotation_matrix  # Insert the rotation matrix\n",
    "    T[:3, 3] = xyz  # Insert the translation vector\n",
    "\n",
    "    return T\n",
    "\n",
    "demo = [\n",
    "    0.5041812568964179,\n",
    "    0.09185436015924689,\n",
    "    0.48143709451847916,\n",
    "    -0.9983786626178943,\n",
    "    0.013449729176136651,\n",
    "    -0.054892707858185244,\n",
    "    0.006778011389003352\n",
    "]\n",
    "\n",
    "T_eef = create_homogeneous_matrix(demo[:3], demo[3:])\n",
    "T_eef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.80887007,  0.58083391,  0.09143946,  0.62268341],\n",
       "       [ 0.57427787, -0.81378487,  0.08921382, -0.0129589 ],\n",
       "       [ 0.12623046, -0.01965073, -0.99180629,  0.48391332],\n",
       "       [ 0.        ,  0.        ,  0.        ,  1.        ]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T_delta_world @ T_eef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.74345614, -0.66244621,  0.09185851,  0.62268341],\n",
       "       [-0.65977202, -0.74895908, -0.06132849, -0.0129589 ],\n",
       "       [ 0.10942509, -0.01501063, -0.9938817 ,  0.48391332],\n",
       "       [ 0.        ,  0.        ,  0.        ,  1.        ]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_T @ T_eef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.76608952,  0.64273389,  0.        ,  0.28067213],\n",
       "       [-0.64273389,  0.76608952, -0.        , -0.38310653],\n",
       "       [-0.        ,  0.        ,  1.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  1.        ]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T_bias = create_homogeneous_matrix([trans[0], trans[1], 0], R.from_euler('xyz', [0, 0, rotation[2]]).as_quat())\n",
    "T_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.12649077, -0.10964465, -0.02777028])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R.from_matrix(T_eef[:3, :3]).as_euler('xyz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.12649077, -0.10964465, -0.72583185])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = T_bias @ T_eef\n",
    "R.from_matrix(res[:3, :3]).as_euler('xyz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.74345614, -0.66244621,  0.09185851],\n",
       "       [-0.65977202, -0.74895908, -0.06132849],\n",
       "       [ 0.10942509, -0.01501063, -0.9938817 ]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R.from_euler('xyz', [0, 0, rotation[2]]).as_matrix() @ T_eef[:3, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pose_inv(pose):\n",
    "    \"\"\"Inverse a 4x4 homogeneous transformation matrix.\"\"\"\n",
    "    R = pose[:3, :3]\n",
    "    T = np.eye(4)\n",
    "    T[:3, :3] = R.T\n",
    "    T[:3, 3] = - R.T @ np.ascontiguousarray(pose[:3, 3])\n",
    "    return T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8.04888450e-01, -6.66575509e-17,  5.93426139e-01],\n",
       "       [-4.17340537e-01,  7.10919961e-01,  5.66056256e-01],\n",
       "       [-4.21878488e-01, -7.03272926e-01,  5.72211266e-01]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "r_project = np.linalg.inv(R.from_euler('xyz', [rotation[0], rotation[1], rotation[2]]).as_matrix()) @ R.from_euler('xyz', [0, 0, rotation[2]]).as_matrix()\n",
    "r_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.76608952,  0.64273389,  0.        ],\n",
       "       [-0.64273389,  0.76608952, -0.        ],\n",
       "       [-0.        ,  0.        ,  1.        ]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = R.from_euler('xyz', [0, 0, rotation[-1]])\n",
    "\n",
    "# Get the rotation matrix\n",
    "adjusted_rotation_matrix = r.as_matrix()\n",
    "\n",
    "adjusted_rotation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.17739752, 0.24072682, 0.00247622])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjusted_translation = T_delta_world[:3, :3] @ T_eef[:3, 3] - adjusted_rotation_matrix @ T_eef[:3, 3] + T_delta_world[:3, 3]\n",
    "adjusted_translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.76608952,  0.64273389,  0.        ,  0.17739752],\n",
       "       [-0.64273389,  0.76608952, -0.        ,  0.24072682],\n",
       "       [-0.        ,  0.        ,  1.        ,  0.00247622],\n",
       "       [ 0.        ,  0.        ,  0.        ,  1.        ]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_T =  create_homogeneous_matrix(adjusted_translation, r.as_quat())\n",
    "new_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.79857538, -0.60179267, -0.011088  ,  0.28067213],\n",
       "       [ 0.60165335,  0.79864132, -0.01361215, -0.38310653],\n",
       "       [ 0.01704703,  0.00419919,  0.99984587, -0.00643008],\n",
       "       [ 0.        ,  0.        ,  0.        ,  1.        ]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T_delta_world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted rotation -1854.694161068739\n",
      "[-0.09844735 -0.39142355 -0.21084177]\n",
      "[[ 0.57794079  0.8160787   0.         -0.09844735]\n",
      " [-0.8160787   0.57794079  0.         -0.39142355]\n",
      " [ 0.          0.          1.         -0.21084177]\n",
      " [ 0.          0.          0.          1.        ]]\n"
     ]
    }
   ],
   "source": [
    "PointCloud = np.ndarray\n",
    "\n",
    "def rotate_pointcloud(pcd: PointCloud, angle_z: float):\n",
    "    print(\"predicted rotation\", np.rad2deg(angle_z))\n",
    "    R = np.eye(3)\n",
    "    cosine = np.cos(angle_z)\n",
    "    sine = np.sin(angle_z)\n",
    "    R[0, 0] = cosine\n",
    "    R[1, 1] = cosine\n",
    "    R[0, 1] = -sine\n",
    "    R[1, 0] = sine\n",
    "\n",
    "    pcd[:3, :] = R @ pcd[:3, :]\n",
    "    return R, pcd\n",
    "\n",
    "def find_translation(pcd0: PointCloud, pcd1: PointCloud) -> np.ndarray:\n",
    "    pcd0_centre = np.mean(pcd0[:3, :], axis=1)\n",
    "    pcd1_centre = np.mean(pcd1[:3, :], axis=1)\n",
    "    return pcd1_centre - pcd0_centre\n",
    "    \n",
    "R_mtx, rotated_pcd0 = rotate_pointcloud(data[\"pc0\"], rotation[-1])\n",
    "translation = find_translation(rotated_pcd0, data[\"pc1\"])\n",
    "\n",
    "print(translation)\n",
    "T_delta_base = np.eye(4)\n",
    "T_delta_base[:3, :3] = R_mtx\n",
    "T_delta_base[:3, 3] = translation\n",
    "\n",
    "T_delta_cam = pose_inv(data[\"T_WC\"]) @ T_delta_base @ data[\"T_WC\"]\n",
    "\n",
    "print(T_delta_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "\n",
    "width = 848  # Replace with your camera image width\n",
    "height = 480 # Replace with your camera image height\n",
    "fx = 431.56503296\n",
    "fy = 431.18637085\n",
    "cx = 418.71490479\n",
    "cy = 235.15617371 \n",
    "intrinsics = o3d.camera.PinholeCameraIntrinsic(width, height, fx, fy, cx, cy)\n",
    "\n",
    "demo_mask = np.array(Image.open(\"../data/lego/demo_wrist_mask.png\"))\n",
    "demo_rgb = np.array(Image.open(\"../data/lego/demo_wrist_rgb.png\")) \n",
    "demo_depth = np.array(Image.open(\"../data/lego/demo_wrist_depth.png\")).astype(np.uint16)\n",
    "\n",
    "color = o3d.geometry.Image(demo_rgb)\n",
    "depth = o3d.geometry.Image(demo_depth)\n",
    "\n",
    "rgbd_image = o3d.geometry.RGBDImage.create_from_color_and_depth(\n",
    "    color, depth, depth_scale=1000.0, convert_rgb_to_intensity=False)\n",
    "\n",
    "rgbd_image\n",
    "pcd = o3d.geometry.PointCloud.create_from_rgbd_image(rgbd_image, intrinsics) \n",
    "\n",
    "# Visualization\n",
    "o3d.visualization.draw_geometries([pcd]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
