{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from direct.preprocessor import Preprocessor, pose_inv, SceneData\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "import json\n",
    "\n",
    "dir = \"experiments/wood\"\n",
    "\n",
    "demo_head_rgb = np.array(Image.open(\"../{0}/demo_head_rgb.png\".format(dir)))\n",
    "demo_head_depth = np.array(Image.open(\"../{0}/demo_head_depth.png\".format(dir)))\n",
    "demo_head_mask = np.array(Image.open(\"../{0}/demo_head_seg.png\".format(dir)))\n",
    "\n",
    "demo_wrist_rgb = np.array(Image.open(\"../{0}/demo_wrist_rgb.png\".format(dir)))\n",
    "demo_wrist_depth = np.array(Image.open(\"../{0}/demo_wrist_depth.png\".format(dir)))\n",
    "demo_wrist_mask = np.array(Image.open(\"../{0}/demo_wrist_seg.png\".format(dir)))\n",
    "\n",
    "intrinsics_d415 = np.load(\"../handeye/intrinsics_d415.npy\")\n",
    "intrinsics_d405 = np.load(\"../handeye/intrinsics_d405.npy\")\n",
    "\n",
    "data = SceneData(\n",
    "    image_0=demo_head_rgb,\n",
    "    image_1=demo_wrist_rgb,\n",
    "    depth_0=demo_head_depth,\n",
    "    depth_1=demo_wrist_depth,\n",
    "    seg_0=demo_head_mask,\n",
    "    seg_1=demo_wrist_mask,\n",
    "    intrinsics_0=intrinsics_d415,\n",
    "    intrinsics_1=intrinsics_d405,\n",
    "    T_WC=np.eye(4) # cam frame\n",
    ")\n",
    "\n",
    "processor = Preprocessor()\n",
    "data.update(processor(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33m[Open3D WARNING] GLFW Error: Cocoa: Failed to find service port for display\u001b[0;m\n"
     ]
    }
   ],
   "source": [
    "pcd1 = o3d.geometry.PointCloud()\n",
    "pcd1.points = o3d.utility.Vector3dVector(data[\"pc1\"][:, :3])\n",
    "o3d.visualization.draw_geometries([pcd1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42964, 4)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "points_wrist_cam = copy.deepcopy(data[\"pc1\"][:, :3])\n",
    "ones = np.ones((points_wrist_cam.shape[0], 1))\n",
    "\n",
    "# Append the column of ones to the original points array\n",
    "points_wrist_cam_with_ones = np.hstack((points_wrist_cam, ones))\n",
    "\n",
    "points_wrist_cam_with_ones.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.02713561, -0.8344819 ,  1.08403704,  1.        ])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T_C_EEF = np.load(\"../handeye/T_C_EEF_wrist_l.npy\")\n",
    "\n",
    "T_C_EEF @ np.array([1,1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "def create_homogeneous_matrix(xyz, quaternion):\n",
    "    rotation_matrix = R.from_quat(quaternion).as_matrix()\n",
    "    T = np.eye(4)  # Start with an identity matrix\n",
    "    T[:3, :3] = rotation_matrix  # Insert the rotation matrix\n",
    "    T[:3, 3] = xyz  # Insert the translation vector\n",
    "\n",
    "    return T\n",
    "\n",
    "T_WC = np.load(\"../handeye/T_WC_head.npy\")\n",
    "T_C_EEF = np.load(\"../handeye/T_C_EEF_wrist_l.npy\")\n",
    "with open(f\"../{dir}/demo_bottlenecks.json\") as f:\n",
    "    dbn = json.load(f)\n",
    "T_EEF_WORLD = create_homogeneous_matrix(dbn[\"bottleneck_left\"][:3], dbn[\"bottleneck_left\"][3:])\n",
    "\n",
    "T_wrist2head = pose_inv(T_WC) @ T_EEF_WORLD @ T_C_EEF\n",
    "\n",
    "points_head_cam = []\n",
    "for point in points_wrist_cam_with_ones:\n",
    "    points_head_cam.append(T_wrist2head @ point)\n",
    "points_head_cam = np.array(points_head_cam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.05467011, -0.0217233 ,  0.703     ],\n",
       "       [ 0.0554427 , -0.0217233 ,  0.703     ],\n",
       "       [ 0.05621529, -0.0217233 ,  0.703     ],\n",
       "       ...,\n",
       "       [ 0.09847307,  0.13067403,  0.66      ],\n",
       "       [ 0.09934871,  0.13087203,  0.661     ],\n",
       "       [ 0.10007515,  0.13087203,  0.661     ]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"pc0\"][:, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33m[Open3D WARNING] GLFW Error: Cocoa: Failed to find service port for display\u001b[0;m\n",
      "[0.0371766  0.03764415 0.65259916]\n",
      "[0.03624739 0.01021223 0.66775771]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.00092921,  0.02743192, -0.01515856])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pcd0 = o3d.geometry.PointCloud()\n",
    "pcd0.points = o3d.utility.Vector3dVector(data[\"pc0\"][:, :3])\n",
    "pcd1 = o3d.geometry.PointCloud()\n",
    "pcd1.points = o3d.utility.Vector3dVector(points_head_cam[:, :3])\n",
    "o3d.visualization.draw_geometries([pcd0, pcd1])\n",
    "\n",
    "# Calculate the centroid of each point cloud array([-0.00344304,  0.00669954,  0.00428747])\n",
    "pcd0_centre = np.mean(data[\"pc0\"][:, :3], axis=0)  # Calculate mean across columns (axis=0)\n",
    "pcd1_centre = np.mean(points_head_cam[:, :3], axis=0)  # Calculate mean across columns (axis=0)\n",
    "\n",
    "print(pcd0_centre)\n",
    "print(pcd1_centre)\n",
    "# # Compute the difference between the centroids\n",
    "diff = pcd0_centre - pcd1_centre\n",
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run Poisson surface reconstruction\n",
      "3. Colored point cloud registration\n",
      "[50, 0.04, 0]\n",
      "3-1. Downsample with a voxel size 0.04\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'source' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m([\u001b[38;5;28miter\u001b[39m, radius, scale])\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3-1. Downsample with a voxel size \u001b[39m\u001b[38;5;132;01m%.2f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m radius)\n\u001b[0;32m---> 23\u001b[0m source_down \u001b[38;5;241m=\u001b[39m \u001b[43msource\u001b[49m\u001b[38;5;241m.\u001b[39mvoxel_down_sample(radius)\n\u001b[1;32m     24\u001b[0m target_down \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mvoxel_down_sample(radius)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3-2. Estimate normal.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'source' is not defined"
     ]
    }
   ],
   "source": [
    "print('run Poisson surface reconstruction')\n",
    "# Assuming data[\"pc0\"] and data[\"pc1\"] are your point cloud data\n",
    "pcd0 = o3d.geometry.PointCloud()\n",
    "pcd1 = o3d.geometry.PointCloud()\n",
    "\n",
    "pcd0.points = o3d.utility.Vector3dVector(data[\"pc0\"][:, :3])\n",
    "pcd1.points = o3d.utility.Vector3dVector(points_head_cam[:, :3][:, :3])\n",
    "\n",
    "# colored pointcloud registration\n",
    "# This is implementation of following paper\n",
    "# J. Park, Q.-Y. Zhou, V. Koltun,\n",
    "# Colored Point Cloud Registration Revisited, ICCV 2017\n",
    "voxel_radius = [0.04, 0.02, 0.01]\n",
    "max_iter = [50, 30, 14]\n",
    "current_transformation = np.identity(4)\n",
    "print(\"3. Colored point cloud registration\")\n",
    "for scale in range(3):\n",
    "    iter = max_iter[scale]\n",
    "    radius = voxel_radius[scale]\n",
    "    print([iter, radius, scale])\n",
    "\n",
    "    print(\"3-1. Downsample with a voxel size %.2f\" % radius)\n",
    "    source_down = source.voxel_down_sample(radius)\n",
    "    target_down = target.voxel_down_sample(radius)\n",
    "\n",
    "    print(\"3-2. Estimate normal.\")\n",
    "    source_down.estimate_normals(\n",
    "        o3d.geometry.KDTreeSearchParamHybrid(radius=radius * 2, max_nn=30))\n",
    "    target_down.estimate_normals(\n",
    "        o3d.geometry.KDTreeSearchParamHybrid(radius=radius * 2, max_nn=30))\n",
    "\n",
    "    print(\"3-3. Applying colored point cloud registration\")\n",
    "    result_icp = o3d.pipelines.registration.registration_colored_icp(\n",
    "        source_down, target_down, radius, current_transformation,\n",
    "        o3d.pipelines.registration.ICPConvergenceCriteria(relative_fitness=1e-6,\n",
    "                                                          relative_rmse=1e-6,\n",
    "                                                          max_iteration=iter))\n",
    "    current_transformation = result_icp.transformation\n",
    "    print(result_icp)\n",
    "    draw_registration_result_original_color(source, target,\n",
    "                                            result_icp.transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33m[Open3D WARNING] GLFW Error: Cocoa: Failed to find service port for display\u001b[0;m\n",
      "[[ 0.97903383 -0.20356119 -0.00745706 -0.01175871]\n",
      " [ 0.1922788   0.93561638 -0.29605887  0.20526802]\n",
      " [ 0.06724304  0.28841781  0.95514059 -0.03505099]\n",
      " [ 0.          0.          0.          1.        ]]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "import copy\n",
    "import cv2\n",
    "\n",
    "# Assuming data[\"pc0\"] and data[\"pc1\"] are your point cloud data\n",
    "pcd0 = o3d.geometry.PointCloud()\n",
    "pcd1 = o3d.geometry.PointCloud()\n",
    "\n",
    "pcd0.points = o3d.utility.Vector3dVector(data[\"pc0\"][:, :3])\n",
    "pcd1.points = o3d.utility.Vector3dVector(points_head_cam[:, :3])\n",
    "pcd0.colors = o3d.utility.Vector3dVector(data[\"pc0\"][:, 3:6])\n",
    "pcd1.colors = o3d.utility.Vector3dVector(data[\"pc1\"][:, 3:6])\n",
    "\n",
    "# Estimate normals for each point cloud\n",
    "pcd0.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=0.1, max_nn=40))\n",
    "pcd1.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=0.1, max_nn=40))\n",
    "\n",
    "# Function to draw registration results\n",
    "def draw_registration_result(source, target, transformation):\n",
    "    source_temp = copy.deepcopy(source)\n",
    "    target_temp = copy.deepcopy(target)\n",
    "    source_temp.transform(transformation)\n",
    "    source_temp.paint_uniform_color([1, 0.706, 0])\n",
    "    target_temp.paint_uniform_color([0, 0.651, 0.929])\n",
    "    o3d.visualization.draw_geometries([source_temp, target_temp])\n",
    "\n",
    "# Compute FPFH features\n",
    "voxel_size = 0.05  # Set voxel size for downsampling (adjust based on your data)\n",
    "source_down = pcd0.voxel_down_sample(voxel_size)\n",
    "target_down = pcd1.voxel_down_sample(voxel_size)\n",
    "\n",
    "source_down.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=voxel_size * 2, max_nn=30))\n",
    "target_down.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=voxel_size * 2, max_nn=30))\n",
    "\n",
    "source_fpfh = o3d.pipelines.registration.compute_fpfh_feature(\n",
    "    source_down,\n",
    "    o3d.geometry.KDTreeSearchParamHybrid(radius=voxel_size * 5, max_nn=100))\n",
    "\n",
    "target_fpfh = o3d.pipelines.registration.compute_fpfh_feature(\n",
    "    target_down,\n",
    "    o3d.geometry.KDTreeSearchParamHybrid(radius=voxel_size * 5, max_nn=100))\n",
    "\n",
    "# Global registration using RANSAC\n",
    "distance_threshold = voxel_size * 1.5\n",
    "result = o3d.pipelines.registration.registration_ransac_based_on_feature_matching(\n",
    "    source_down, target_down, source_fpfh, target_fpfh, mutual_filter=False,\n",
    "    max_correspondence_distance=distance_threshold,\n",
    "    estimation_method=o3d.pipelines.registration.TransformationEstimationPointToPoint(), \n",
    "    ransac_n=4,\n",
    "    checkers=[\n",
    "        o3d.pipelines.registration.CorrespondenceCheckerBasedOnEdgeLength(0.9), \n",
    "        o3d.pipelines.registration.CorrespondenceCheckerBasedOnDistance(distance_threshold)\n",
    "    ],\n",
    "    criteria=o3d.pipelines.registration.RANSACConvergenceCriteria(4000000, 500)\n",
    ")\n",
    "\n",
    "# Use the result of global registration as the initial transformation for ICP\n",
    "trans_init = result.transformation\n",
    "\n",
    "# Apply ICP\n",
    "threshold = 0.01  # Set a threshold for ICP, this depends on your data\n",
    "reg_p2p = o3d.pipelines.registration.registration_icp(\n",
    "    pcd0, pcd1, threshold, trans_init,\n",
    "    o3d.pipelines.registration.TransformationEstimationPointToPoint())\n",
    "\n",
    "# Get the transformation matrix\n",
    "T_delta_cam = reg_p2p.transformation\n",
    "\n",
    "# Draw the result\n",
    "draw_registration_result(pcd0, pcd1, T_delta_cam)\n",
    "\n",
    "print(T_delta_cam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.02814375, -0.06392402,  0.59424258]),\n",
       " array([ 0.62529498,  0.60027432, -0.34746441, -0.35769457]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def translation_from_matrix(matrix):\n",
    "    \"\"\"Extracts the translation vector from a 4x4 homogeneous transformation matrix.\"\"\"\n",
    "    return matrix[:3, 3]\n",
    "\n",
    "def quaternion_from_matrix(matrix):\n",
    "    \"\"\"Extracts the quaternion from a 4x4 homogeneous transformation matrix.\"\"\"\n",
    "    rotation_matrix = matrix[:3, :3]\n",
    "    rotation = R.from_matrix(rotation_matrix)\n",
    "    return rotation.as_quat()\n",
    "\n",
    "translation_from_matrix(T_WC_new), quaternion_from_matrix(T_WC_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. Colored point cloud registration\n",
      "[50, 0.04, 0]\n",
      "3-1. Downsample with a voxel size 0.04\n",
      "3-2. Estimate normal.\n",
      "3-3. Applying colored point cloud registration\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "registration_colored_icp(): incompatible function arguments. The following argument types are supported:\n    1. (source: open3d.cpu.pybind.geometry.PointCloud, target: open3d.cpu.pybind.geometry.PointCloud, max_correspondence_distance: float, init: numpy.ndarray[numpy.float64[4, 4]] = array([[1., 0., 0., 0.],\n       [0., 1., 0., 0.],\n       [0., 0., 1., 0.],\n       [0., 0., 0., 1.]]), estimation_method: open3d.cpu.pybind.pipelines.registration.TransformationEstimationForColoredICP = TransformationEstimationForColoredICP with lambda_geometric=0.968000, criteria: open3d.cpu.pybind.pipelines.registration.ICPConvergenceCriteria = ICPConvergenceCriteria class with relative_fitness=1.000000e-06, relative_rmse=1.000000e-06, and max_iteration=30) -> open3d.cpu.pybind.pipelines.registration.RegistrationResult\n\nInvoked with: PointCloud with 16 points., PointCloud with 16 points., 0.04, array([[1., 0., 0., 0.],\n       [0., 1., 0., 0.],\n       [0., 0., 1., 0.],\n       [0., 0., 0., 1.]]), ICPConvergenceCriteria class with relative_fitness=1.000000e-06, relative_rmse=1.000000e-06, and max_iteration=50",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[112], line 25\u001b[0m\n\u001b[1;32m     21\u001b[0m target_down\u001b[38;5;241m.\u001b[39mestimate_normals(\n\u001b[1;32m     22\u001b[0m     o3d\u001b[38;5;241m.\u001b[39mgeometry\u001b[38;5;241m.\u001b[39mKDTreeSearchParamHybrid(radius\u001b[38;5;241m=\u001b[39mradius \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, max_nn\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m))\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3-3. Applying colored point cloud registration\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m result_icp \u001b[38;5;241m=\u001b[39m \u001b[43mo3d\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipelines\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregistration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregistration_colored_icp\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43msource_down\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_down\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mradius\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_transformation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mo3d\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipelines\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregistration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mICPConvergenceCriteria\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrelative_fitness\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m                                                      \u001b[49m\u001b[43mrelative_rmse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m                                                      \u001b[49m\u001b[43mmax_iteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m current_transformation \u001b[38;5;241m=\u001b[39m result_icp\u001b[38;5;241m.\u001b[39mtransformation\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(result_icp)\n",
      "\u001b[0;31mTypeError\u001b[0m: registration_colored_icp(): incompatible function arguments. The following argument types are supported:\n    1. (source: open3d.cpu.pybind.geometry.PointCloud, target: open3d.cpu.pybind.geometry.PointCloud, max_correspondence_distance: float, init: numpy.ndarray[numpy.float64[4, 4]] = array([[1., 0., 0., 0.],\n       [0., 1., 0., 0.],\n       [0., 0., 1., 0.],\n       [0., 0., 0., 1.]]), estimation_method: open3d.cpu.pybind.pipelines.registration.TransformationEstimationForColoredICP = TransformationEstimationForColoredICP with lambda_geometric=0.968000, criteria: open3d.cpu.pybind.pipelines.registration.ICPConvergenceCriteria = ICPConvergenceCriteria class with relative_fitness=1.000000e-06, relative_rmse=1.000000e-06, and max_iteration=30) -> open3d.cpu.pybind.pipelines.registration.RegistrationResult\n\nInvoked with: PointCloud with 16 points., PointCloud with 16 points., 0.04, array([[1., 0., 0., 0.],\n       [0., 1., 0., 0.],\n       [0., 0., 1., 0.],\n       [0., 0., 0., 1.]]), ICPConvergenceCriteria class with relative_fitness=1.000000e-06, relative_rmse=1.000000e-06, and max_iteration=50"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# colored pointcloud registration\n",
    "# This is implementation of following paper\n",
    "# J. Park, Q.-Y. Zhou, V. Koltun,\n",
    "# Colored Point Cloud Registration Revisited, ICCV 2017\n",
    "voxel_radius = [0.04, 0.02, 0.01]\n",
    "max_iter = [50, 30, 14]\n",
    "current_transformation = np.identity(4)\n",
    "print(\"3. Colored point cloud registration\")\n",
    "for scale in range(3):\n",
    "    iter = max_iter[scale]\n",
    "    radius = voxel_radius[scale]\n",
    "    print([iter, radius, scale])\n",
    "\n",
    "    print(\"3-1. Downsample with a voxel size %.2f\" % radius)\n",
    "    # source_down = source.voxel_down_sample(radius)\n",
    "    # target_down = target.voxel_down_sample(radius)\n",
    "\n",
    "    print(\"3-2. Estimate normal.\")\n",
    "    source_down.estimate_normals(\n",
    "        o3d.geometry.KDTreeSearchParamHybrid(radius=radius * 2, max_nn=30))\n",
    "    target_down.estimate_normals(\n",
    "        o3d.geometry.KDTreeSearchParamHybrid(radius=radius * 2, max_nn=30))\n",
    "\n",
    "    print(\"3-3. Applying colored point cloud registration\")\n",
    "    result_icp = o3d.pipelines.registration.registration_colored_icp(\n",
    "        source_down, target_down, radius, current_transformation,\n",
    "        o3d.pipelines.registration.ICPConvergenceCriteria(relative_fitness=1e-6,\n",
    "                                                          relative_rmse=1e-6,\n",
    "                                                          max_iteration=iter))\n",
    "    current_transformation = result_icp.transformation\n",
    "    print(result_icp)\n",
    "\n",
    "# Get the transformation matrix\n",
    "T_delta_cam = reg_p2p.transformation\n",
    "\n",
    "# Draw the result\n",
    "draw_registration_result(pcd0, pcd1, T_delta_cam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.79857538, -0.60179267, -0.011088  ,  0.28067213],\n",
       "       [ 0.60165335,  0.79864132, -0.01361215, -0.38310653],\n",
       "       [ 0.01704703,  0.00419919,  0.99984587, -0.00643008],\n",
       "       [ 0.        ,  0.        ,  0.        ,  1.        ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T_WC = np.load(\"../handeye/T_WC_head.npy\")\n",
    "T_delta_world = T_WC @ T_delta_cam @ pose_inv(T_WC)\n",
    "T_delta_world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.79869144, -0.60174079,  0.        ],\n",
       "       [ 0.60174079,  0.79869144,  0.        ],\n",
       "       [ 0.        ,  0.        ,  1.        ]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = R.from_matrix(T_delta_world[:3, :3]).as_euler(\"xyz\")\n",
    "yaw_only_delta_rotation = R.from_euler(\"xyz\", [0, 0, r[-1]]).as_matrix()\n",
    "yaw_only_delta_rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.28067213 -0.38310653 -0.00643008] [ 0.77999087 -0.63530886 37.00105027]\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "\n",
    "def translation_from_matrix(matrix):\n",
    "    \"\"\"Extracts the translation vector from a 4x4 homogeneous transformation matrix.\"\"\"\n",
    "    return matrix[:3, 3]\n",
    "\n",
    "def euler_from_matrix(matrix):\n",
    "    \"\"\"Extracts the quaternion from a 4x4 homogeneous transformation matrix.\"\"\"\n",
    "    rotation_matrix = matrix[:3, :3].copy()\n",
    "    rotation = R.from_matrix(rotation_matrix)\n",
    "    return rotation.as_euler(seq=\"XYZ\", degrees=True)\n",
    "\n",
    "trans = translation_from_matrix(T_delta_world)\n",
    "rotation = euler_from_matrix(T_delta_world)\n",
    "\n",
    "print(trans, rotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.99361179, -0.02611172,  0.10978974,  0.50418126],\n",
       "       [-0.02759997, -0.99954633,  0.01205746,  0.09185436],\n",
       "       [ 0.10942509, -0.01501063, -0.9938817 ,  0.48143709],\n",
       "       [ 0.        ,  0.        ,  0.        ,  1.        ]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_homogeneous_matrix(xyz, quaternion):\n",
    "    # Convert the quaternion to a rotation matrix\n",
    "    rotation_matrix = R.from_quat(quaternion).as_matrix()\n",
    "    # Create a homogeneous transformation matrix\n",
    "    T = np.eye(4)  # Start with an identity matrix\n",
    "    T[:3, :3] = rotation_matrix  # Insert the rotation matrix\n",
    "    T[:3, 3] = xyz  # Insert the translation vector\n",
    "\n",
    "    return T\n",
    "\n",
    "demo = [\n",
    "    0.5041812568964179,\n",
    "    0.09185436015924689,\n",
    "    0.48143709451847916,\n",
    "    -0.9983786626178943,\n",
    "    0.013449729176136651,\n",
    "    -0.054892707858185244,\n",
    "    0.006778011389003352\n",
    "]\n",
    "\n",
    "T_eef = create_homogeneous_matrix(demo[:3], demo[3:])\n",
    "T_eef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.80887007,  0.58083391,  0.09143946,  0.62268341],\n",
       "       [ 0.57427787, -0.81378487,  0.08921382, -0.0129589 ],\n",
       "       [ 0.12623046, -0.01965073, -0.99180629,  0.48391332],\n",
       "       [ 0.        ,  0.        ,  0.        ,  1.        ]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T_delta_world @ T_eef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.74345614, -0.66244621,  0.09185851,  0.62268341],\n",
       "       [-0.65977202, -0.74895908, -0.06132849, -0.0129589 ],\n",
       "       [ 0.10942509, -0.01501063, -0.9938817 ,  0.48391332],\n",
       "       [ 0.        ,  0.        ,  0.        ,  1.        ]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_T @ T_eef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.76608952,  0.64273389,  0.        ,  0.28067213],\n",
       "       [-0.64273389,  0.76608952, -0.        , -0.38310653],\n",
       "       [-0.        ,  0.        ,  1.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  1.        ]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T_bias = create_homogeneous_matrix([trans[0], trans[1], 0], R.from_euler('xyz', [0, 0, rotation[2]]).as_quat())\n",
    "T_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.12649077, -0.10964465, -0.02777028])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R.from_matrix(T_eef[:3, :3]).as_euler('xyz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.12649077, -0.10964465, -0.72583185])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = T_bias @ T_eef\n",
    "R.from_matrix(res[:3, :3]).as_euler('xyz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.74345614, -0.66244621,  0.09185851],\n",
       "       [-0.65977202, -0.74895908, -0.06132849],\n",
       "       [ 0.10942509, -0.01501063, -0.9938817 ]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R.from_euler('xyz', [0, 0, rotation[2]]).as_matrix() @ T_eef[:3, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pose_inv(pose):\n",
    "    \"\"\"Inverse a 4x4 homogeneous transformation matrix.\"\"\"\n",
    "    R = pose[:3, :3]\n",
    "    T = np.eye(4)\n",
    "    T[:3, :3] = R.T\n",
    "    T[:3, 3] = - R.T @ np.ascontiguousarray(pose[:3, 3])\n",
    "    return T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8.04888450e-01, -6.66575509e-17,  5.93426139e-01],\n",
       "       [-4.17340537e-01,  7.10919961e-01,  5.66056256e-01],\n",
       "       [-4.21878488e-01, -7.03272926e-01,  5.72211266e-01]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "r_project = np.linalg.inv(R.from_euler('xyz', [rotation[0], rotation[1], rotation[2]]).as_matrix()) @ R.from_euler('xyz', [0, 0, rotation[2]]).as_matrix()\n",
    "r_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.76608952,  0.64273389,  0.        ],\n",
       "       [-0.64273389,  0.76608952, -0.        ],\n",
       "       [-0.        ,  0.        ,  1.        ]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = R.from_euler('xyz', [0, 0, rotation[-1]])\n",
    "\n",
    "# Get the rotation matrix\n",
    "adjusted_rotation_matrix = r.as_matrix()\n",
    "\n",
    "adjusted_rotation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.17739752, 0.24072682, 0.00247622])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjusted_translation = T_delta_world[:3, :3] @ T_eef[:3, 3] - adjusted_rotation_matrix @ T_eef[:3, 3] + T_delta_world[:3, 3]\n",
    "adjusted_translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.76608952,  0.64273389,  0.        ,  0.17739752],\n",
       "       [-0.64273389,  0.76608952, -0.        ,  0.24072682],\n",
       "       [-0.        ,  0.        ,  1.        ,  0.00247622],\n",
       "       [ 0.        ,  0.        ,  0.        ,  1.        ]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_T =  create_homogeneous_matrix(adjusted_translation, r.as_quat())\n",
    "new_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.79857538, -0.60179267, -0.011088  ,  0.28067213],\n",
       "       [ 0.60165335,  0.79864132, -0.01361215, -0.38310653],\n",
       "       [ 0.01704703,  0.00419919,  0.99984587, -0.00643008],\n",
       "       [ 0.        ,  0.        ,  0.        ,  1.        ]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T_delta_world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted rotation -1854.694161068739\n",
      "[-0.09844735 -0.39142355 -0.21084177]\n",
      "[[ 0.57794079  0.8160787   0.         -0.09844735]\n",
      " [-0.8160787   0.57794079  0.         -0.39142355]\n",
      " [ 0.          0.          1.         -0.21084177]\n",
      " [ 0.          0.          0.          1.        ]]\n"
     ]
    }
   ],
   "source": [
    "PointCloud = np.ndarray\n",
    "\n",
    "def rotate_pointcloud(pcd: PointCloud, angle_z: float):\n",
    "    print(\"predicted rotation\", np.rad2deg(angle_z))\n",
    "    R = np.eye(3)\n",
    "    cosine = np.cos(angle_z)\n",
    "    sine = np.sin(angle_z)\n",
    "    R[0, 0] = cosine\n",
    "    R[1, 1] = cosine\n",
    "    R[0, 1] = -sine\n",
    "    R[1, 0] = sine\n",
    "\n",
    "    pcd[:3, :] = R @ pcd[:3, :]\n",
    "    return R, pcd\n",
    "\n",
    "def find_translation(pcd0: PointCloud, pcd1: PointCloud) -> np.ndarray:\n",
    "    pcd0_centre = np.mean(pcd0[:3, :], axis=1)\n",
    "    pcd1_centre = np.mean(pcd1[:3, :], axis=1)\n",
    "    return pcd1_centre - pcd0_centre\n",
    "    \n",
    "R_mtx, rotated_pcd0 = rotate_pointcloud(data[\"pc0\"], rotation[-1])\n",
    "translation = find_translation(rotated_pcd0, data[\"pc1\"])\n",
    "\n",
    "print(translation)\n",
    "T_delta_base = np.eye(4)\n",
    "T_delta_base[:3, :3] = R_mtx\n",
    "T_delta_base[:3, 3] = translation\n",
    "\n",
    "T_delta_cam = pose_inv(data[\"T_WC\"]) @ T_delta_base @ data[\"T_WC\"]\n",
    "\n",
    "print(T_delta_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "\n",
    "width = 848  # Replace with your camera image width\n",
    "height = 480 # Replace with your camera image height\n",
    "fx = 431.56503296\n",
    "fy = 431.18637085\n",
    "cx = 418.71490479\n",
    "cy = 235.15617371 \n",
    "intrinsics = o3d.camera.PinholeCameraIntrinsic(width, height, fx, fy, cx, cy)\n",
    "\n",
    "demo_mask = np.array(Image.open(\"../data/lego/demo_wrist_mask.png\"))\n",
    "demo_rgb = np.array(Image.open(\"../data/lego/demo_wrist_rgb.png\")) \n",
    "demo_depth = np.array(Image.open(\"../data/lego/demo_wrist_depth.png\")).astype(np.uint16)\n",
    "\n",
    "color = o3d.geometry.Image(demo_rgb)\n",
    "depth = o3d.geometry.Image(demo_depth)\n",
    "\n",
    "rgbd_image = o3d.geometry.RGBDImage.create_from_color_and_depth(\n",
    "    color, depth, depth_scale=1000.0, convert_rgb_to_intensity=False)\n",
    "\n",
    "rgbd_image\n",
    "pcd = o3d.geometry.PointCloud.create_from_rgbd_image(rgbd_image, intrinsics) \n",
    "\n",
    "# Visualization\n",
    "o3d.visualization.draw_geometries([pcd]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
